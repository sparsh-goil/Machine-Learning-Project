{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from metadata file\n",
    "file=open(\"/media/spaggy/New Volume/Sem VI/ML/Project/YelpZip/metadata\")\n",
    "data=file.readlines()\n",
    "file.close()\n",
    "for ind in range(len(data)):\n",
    "    data[ind]=data[ind].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reviews from reviewcontent file\n",
    "file2=open(\"/media/spaggy/New Volume/Sem VI/ML/Project/YelpZip/reviewContent\")\n",
    "text=file2.readlines()\n",
    "file2.close()\n",
    "for i in range(len(text)):\n",
    "    text[i]=text[i].split(\"\\t\")\n",
    "text=[t[3] for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dataset=pd.DataFrame(data,columns=[\"user_id\",'product_id','rating','label','date'])\n",
    "Dataset[\"review_text\"]=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(608598, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dataset['rating']=Dataset['rating'].astype('float')\n",
    "Dataset['date']=pd.to_datetime(Dataset['date'])\n",
    "Dataset['day']=Dataset['date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5044</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-11-16</td>\n",
       "      <td>Drinks were bad, the hot chocolate was watered...</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5045</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-09-08</td>\n",
       "      <td>This was the worst experience I've ever had a ...</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5046</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2013-10-06</td>\n",
       "      <td>This is located on the site of the old Spruce ...</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5047</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-11-30</td>\n",
       "      <td>I enjoyed coffee and breakfast twice at Toast ...</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5048</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-08-28</td>\n",
       "      <td>I love Toast! The food choices are fantastic -...</td>\n",
       "      <td>Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id product_id  rating label       date  \\\n",
       "0    5044          0     1.0    -1 2014-11-16   \n",
       "1    5045          0     1.0    -1 2014-09-08   \n",
       "2    5046          0     3.0    -1 2013-10-06   \n",
       "3    5047          0     5.0    -1 2014-11-30   \n",
       "4    5048          0     5.0    -1 2014-08-28   \n",
       "\n",
       "                                         review_text       day  \n",
       "0  Drinks were bad, the hot chocolate was watered...    Sunday  \n",
       "1  This was the worst experience I've ever had a ...    Monday  \n",
       "2  This is located on the site of the old Spruce ...    Sunday  \n",
       "3  I enjoyed coffee and breakfast twice at Toast ...    Sunday  \n",
       "4  I love Toast! The food choices are fantastic -...  Thursday  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame({'label':Dataset[\"label\"]})\n",
    "x=Dataset.drop(['label'],axis=1)\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Crafted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------review_centric features extracted-------------/\n",
      "-----------user_centric features extracted-------------/\n",
      "-----------product_centric features extracted-------------/\n",
      "\n",
      "\n",
      "-----------review_centric features extracted-------------/\n",
      "-----------user_centric features extracted-------------/\n",
      "-----------product_centric features extracted-------------/\n"
     ]
    }
   ],
   "source": [
    "# X_train['upper_case_word_count']=X_train['review_text'].apply(lambda x:len([y for y in x.split() if y) )\n",
    "#Train Dataset\n",
    "#Review centric features\n",
    "X_train['word_count']=X_train['review_text'].apply(lambda x: len(x.split()))\n",
    "X_train['punctuation_count']=X_train['review_text'].apply(lambda x: len(''.join(c for c in x if c in string.punctuation)))\n",
    "X_train['char_count']=X_train['review_text'].apply(lambda x: len(x))\n",
    "X_train['title_count']=X_train['review_text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
    "print(\"-----------review_centric features extracted-------------/\")\n",
    "# user-centric features\n",
    "X_train['user_id_no_of_review'] = X_train.groupby('user_id')['user_id'].transform('size')\n",
    "X_train['user_id_ave_rating'] = X_train.groupby('user_id')['rating'].transform('mean')\n",
    "X_train['user_id_ave_no_words'] = X_train.groupby('user_id')['word_count'].transform('mean')\n",
    "X_train['user_id_max_review_a_day'] = X_train.groupby(['user_id','day'])['user_id'].transform('size')\n",
    "print(\"-----------user_centric features extracted-------------/\")\n",
    "#Product centric feature\n",
    "X_train['product_id_no_of_review']=X_train.groupby('product_id')['product_id'].transform('size')\n",
    "X_train['product_id_ave_rating']=X_train.groupby('product_id')['rating'].transform('mean')\n",
    "X_train['product_id_ave_no_of_words']=X_train.groupby('product_id')['word_count'].transform('mean')\n",
    "X_train['product_id_max_review_a_day']=X_train.groupby(['product_id','day'])['user_id'].transform('size')\n",
    "print(\"-----------product_centric features extracted-------------/\\n\\n\")\n",
    "\n",
    "#Test dataset\n",
    "X_test['word_count']=X_test['review_text'].apply(lambda x: len(x.split()))\n",
    "X_test['punctuation_count']=X_test['review_text'].apply(lambda x: len(''.join(c for c in x if c in string.punctuation)))\n",
    "X_test['char_count']=X_test['review_text'].apply(lambda x: len(x))\n",
    "X_test['title_count']=X_test['review_text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
    "print(\"-----------review_centric features extracted-------------/\")\n",
    "# user-centric features\n",
    "X_test['user_id_no_of_review'] = X_test.groupby('user_id')['user_id'].transform('size')\n",
    "X_test['user_id_ave_rating'] = X_test.groupby('user_id')['rating'].transform('mean')\n",
    "X_test['user_id_ave_no_words'] = X_test.groupby('user_id')['word_count'].transform('mean')\n",
    "X_test['user_id_max_review_a_day'] = X_test.groupby(['user_id','day'])['user_id'].transform('size')\n",
    "print(\"-----------user_centric features extracted-------------/\")\n",
    "#Product centric feature\n",
    "X_test['product_id_no_of_review']=X_test.groupby('product_id')['product_id'].transform('size')\n",
    "X_test['product_id_ave_rating']=X_test.groupby('product_id')['rating'].transform('mean')\n",
    "X_test['product_id_ave_no_of_words']=X_test.groupby('product_id')['word_count'].transform('mean')\n",
    "X_test['product_id_max_review_a_day']=X_test.groupby(['product_id','day'])['user_id'].transform('size')\n",
    "print(\"-----------product_centric features extracted-------------/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>day</th>\n",
       "      <th>word_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>title_count</th>\n",
       "      <th>user_id_no_of_review</th>\n",
       "      <th>user_id_ave_rating</th>\n",
       "      <th>user_id_ave_no_words</th>\n",
       "      <th>user_id_max_review_a_day</th>\n",
       "      <th>product_id_no_of_review</th>\n",
       "      <th>product_id_ave_rating</th>\n",
       "      <th>product_id_ave_no_of_words</th>\n",
       "      <th>product_id_max_review_a_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>353682</th>\n",
       "      <td>185036</td>\n",
       "      <td>3085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-02-20</td>\n",
       "      <td>No stars. One of the most overrated new hipste...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>4.202247</td>\n",
       "      <td>102.719101</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313172</th>\n",
       "      <td>41781</td>\n",
       "      <td>1433</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2014-04-15</td>\n",
       "      <td>Came here last Friday for a wedding rehearsal ...</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>78</td>\n",
       "      <td>10</td>\n",
       "      <td>419</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4.60</td>\n",
       "      <td>73.4</td>\n",
       "      <td>3</td>\n",
       "      <td>875</td>\n",
       "      <td>3.864000</td>\n",
       "      <td>129.496000</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105323</th>\n",
       "      <td>34537</td>\n",
       "      <td>4922</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2014-07-09</td>\n",
       "      <td>Dear Cucina Zapata, Please forgive me, for I h...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>206</td>\n",
       "      <td>26</td>\n",
       "      <td>1087</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>4.00</td>\n",
       "      <td>244.0</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>4.708333</td>\n",
       "      <td>121.833333</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338630</th>\n",
       "      <td>46438</td>\n",
       "      <td>2940</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2007-12-23</td>\n",
       "      <td>Yes, the name is the address. It may take a li...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>178</td>\n",
       "      <td>20</td>\n",
       "      <td>922</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>104.5</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>194.523810</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126404</th>\n",
       "      <td>36282</td>\n",
       "      <td>1153</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2014-12-14</td>\n",
       "      <td>Had high hopes for brunch at Cafe Ghia, but I ...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>68</td>\n",
       "      <td>13</td>\n",
       "      <td>368</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>100.5</td>\n",
       "      <td>3</td>\n",
       "      <td>181</td>\n",
       "      <td>3.883978</td>\n",
       "      <td>101.928177</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id product_id  rating       date  \\\n",
       "353682  185036       3085     1.0 2013-02-20   \n",
       "313172   41781       1433     5.0 2014-04-15   \n",
       "105323   34537       4922     5.0 2014-07-09   \n",
       "338630   46438       2940     4.0 2007-12-23   \n",
       "126404   36282       1153     3.0 2014-12-14   \n",
       "\n",
       "                                              review_text        day  \\\n",
       "353682  No stars. One of the most overrated new hipste...  Wednesday   \n",
       "313172  Came here last Friday for a wedding rehearsal ...    Tuesday   \n",
       "105323  Dear Cucina Zapata, Please forgive me, for I h...  Wednesday   \n",
       "338630  Yes, the name is the address. It may take a li...     Sunday   \n",
       "126404  Had high hopes for brunch at Cafe Ghia, but I ...     Sunday   \n",
       "\n",
       "        word_count  punctuation_count  char_count  title_count  \\\n",
       "353682          12                  2          68            3   \n",
       "313172          78                 10         419            6   \n",
       "105323         206                 26        1087           27   \n",
       "338630         178                 20         922           14   \n",
       "126404          68                 13         368           10   \n",
       "\n",
       "        user_id_no_of_review  user_id_ave_rating  user_id_ave_no_words  \\\n",
       "353682                     1                1.00                  12.0   \n",
       "313172                     5                4.60                  73.4   \n",
       "105323                     3                4.00                 244.0   \n",
       "338630                     4                3.25                 104.5   \n",
       "126404                     4                4.50                 100.5   \n",
       "\n",
       "        user_id_max_review_a_day  product_id_no_of_review  \\\n",
       "353682                         1                      178   \n",
       "313172                         3                      875   \n",
       "105323                         1                       96   \n",
       "338630                         3                       42   \n",
       "126404                         3                      181   \n",
       "\n",
       "        product_id_ave_rating  product_id_ave_no_of_words  \\\n",
       "353682               4.202247                  102.719101   \n",
       "313172               3.864000                  129.496000   \n",
       "105323               4.708333                  121.833333   \n",
       "338630               4.761905                  194.523810   \n",
       "126404               3.883978                  101.928177   \n",
       "\n",
       "        product_id_max_review_a_day  \n",
       "353682                           22  \n",
       "313172                          119  \n",
       "105323                           20  \n",
       "338630                            8  \n",
       "126404                           38  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for expanding all the contractions in the paragraph\n",
    "with open(\"Contractions.json\",'r') as file:\n",
    "   Contractions=json.load(file)\n",
    "c_re = re.compile('(%s)' % '|'.join(Contractions.keys()))\n",
    "\n",
    "def expandContractions(text, c_re=c_re):\n",
    "    def replace(match):\n",
    "        return Contractions[match.group(0)]\n",
    "    return c_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lowercase,remove digits,punctuations,expand contractions,stopwords and lemmatization\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "X_train['review_text']=X_train['review_text'].apply(lambda x: x.decode('utf-8','ignore'))\n",
    "X_train['review_text']=X_train['review_text'].apply(lambda x: expandContractions(x))\n",
    "X_train['review_text']=X_train['review_text'].apply(lambda x:x.lower())   #Convert into lowercase\n",
    "X_train['review_text']=X_train['review_text'].apply(lambda x : ''.join([c for c in x if not c.isdigit()])) #Remove numeric digits\n",
    "X_train['review_text']=X_train['review_text'].apply(lambda x:  ''.join([c for c in x if c not in string.punctuation]))  # Remove punctuations\n",
    "X_train['review_text']=X_train['review_text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "X_train['review_text']=X_train['review_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n",
    "\n",
    "# Test Dataset\n",
    "X_test['review_text']=X_test['review_text'].apply(lambda x: x.decode('utf-8','ignore'))\n",
    "X_test['review_text']=X_test['review_text'].apply(lambda x: expandContractions(x))\n",
    "X_test['review_text']=X_test['review_text'].apply(lambda x:x.lower())   #Convert into lowercase\n",
    "X_test['review_text']=X_test['review_text'].apply(lambda x : ''.join([c for c in x if not c.isdigit()])) #Remove numeric digits\n",
    "X_test['review_text']=X_test['review_text'].apply(lambda x:  ''.join([c for c in x if c not in string.punctuation]))  # Remove punctuations\n",
    "X_test['review_text']=X_test['review_text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "X_test['review_text']=X_test['review_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification on Processed review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count vectorizer for review_text\n",
    "count_vect=CountVectorizer(analyzer='word')\n",
    "count_vect.fit(X_train.review_text)\n",
    "feature_vector_train=count_vect.transform(X_train.review_text)\n",
    "feature_vector_test =count_vect.transform(X_test.review_text)\n",
    "#Count Vectorizer for day\n",
    "count_vect_=CountVectorizer(analyzer='word')\n",
    "count_vect_.fit(X_train.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectors + Features\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"user_id_no_of_review\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"user_id_ave_rating\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"user_id_ave_no_words\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"user_id_max_review_a_day\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"product_id_no_of_review\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"product_id_ave_rating\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"product_id_ave_no_of_words\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"product_id_max_review_a_day\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"word_count\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"punctuation_count\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"char_count\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,np.array(X_train[\"title_count\"])[:,None]))\n",
    "feature_vector_train=hstack((feature_vector_train,count_vect_.transform(X_train[\"day\"])))\n",
    "\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"user_id_no_of_review\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"user_id_ave_rating\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"user_id_ave_no_words\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"user_id_max_review_a_day\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"product_id_no_of_review\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"product_id_ave_rating\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"product_id_ave_no_of_words\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"product_id_max_review_a_day\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"word_count\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"punctuation_count\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"char_count\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,np.array(X_test[\"title_count\"])[:,None]))\n",
    "feature_vector_test=hstack((feature_vector_test,count_vect_.transform(X_test[\"day\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spaggy/.local/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/spaggy/.local/lib/python2.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "LR= LogisticRegression()\n",
    "LR.fit(feature_vector_train,y_train)\n",
    "predictions_train=LR.predict(feature_vector_train)\n",
    "y_pred=LR.predict(feature_vector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<182580x264573 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 14358599 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.65      0.05      0.10     56387\n",
      "           1       0.87      1.00      0.93    369631\n",
      "\n",
      "   micro avg       0.87      0.87      0.87    426018\n",
      "   macro avg       0.76      0.52      0.51    426018\n",
      "weighted avg       0.84      0.87      0.82    426018\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.19      0.41      0.26     24079\n",
      "           1       0.89      0.74      0.81    158501\n",
      "\n",
      "   micro avg       0.70      0.70      0.70    182580\n",
      "   macro avg       0.54      0.58      0.54    182580\n",
      "weighted avg       0.80      0.70      0.74    182580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train,predictions_train))\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of LR on train dataset:', 0.8707378561469233)\n",
      "('Accuracy of LR on test dataset :', 0.6965713659765582)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of LR on train dataset:\",accuracy_score(y_train,predictions_train))\n",
    "print(\"Accuracy of LR on test dataset :\",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification on Processed review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect=CountVectorizer(analyzer='word')\n",
    "# count_vect.fit(X_train.review_text)\n",
    "X_train_count=count_vect.transform(X_train.review_text)\n",
    "X_test_count =count_vect.transform(X_test.review_text)\n",
    "\n",
    "LR= LogisticRegression()\n",
    "LR.fit(X_train_count,y_train)\n",
    "predictions_train=LR.predict(X_train_count)\n",
    "predictions_test=LR.predict(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.88      0.19      0.31     56387\n",
      "           1       0.89      1.00      0.94    369631\n",
      "\n",
      "   micro avg       0.89      0.89      0.89    426018\n",
      "   macro avg       0.89      0.59      0.62    426018\n",
      "weighted avg       0.89      0.89      0.86    426018\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.14      0.49      0.22     24079\n",
      "           1       0.88      0.56      0.68    158501\n",
      "\n",
      "   micro avg       0.55      0.55      0.55    182580\n",
      "   macro avg       0.51      0.52      0.45    182580\n",
      "weighted avg       0.78      0.55      0.62    182580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train,predictions_train))\n",
    "\n",
    "print(classification_report(y_test,predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of LR on train dataset:', 0.8892042120285998)\n",
      "('Accuracy of LR on test dataset :', 0.5489703143827364)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of LR on train dataset:\",accuracy_score(y_train,predictions_train))\n",
    "print(\"Accuracy of LR on test dataset :\",accuracy_score(y_test,predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification on review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vect=CountVectorizer(analyzer='word')\n",
    "count_vect.fit(X_train.review_text)\n",
    "X_train_count=count_vect.transform(X_train.review_text)\n",
    "X_test_count =count_vect.transform(X_test.review_text)\n",
    "\n",
    "LR= LogisticRegression()\n",
    "LR.fit(X_train_count,y_train)\n",
    "predictions_train=LR.predict(X_train_count)\n",
    "predictions_test=LR.predict(X_test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.17      0.29     56193\n",
      "           1       0.89      0.99      0.94    369825\n",
      "\n",
      "   micro avg       0.89      0.89      0.89    426018\n",
      "   macro avg       0.86      0.58      0.61    426018\n",
      "weighted avg       0.88      0.89      0.85    426018\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.40      0.08      0.13     24273\n",
      "           1       0.87      0.98      0.93    158307\n",
      "\n",
      "   micro avg       0.86      0.86      0.86    182580\n",
      "   macro avg       0.64      0.53      0.53    182580\n",
      "weighted avg       0.81      0.86      0.82    182580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train,predictions_train))\n",
    "\n",
    "print(classification_report(y_test,predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8863850823204653\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_train,predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.862115237156315\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame({'label':Dataset[\"label\"]})\n",
    "x=Dataset.drop(['label'],axis=1)\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

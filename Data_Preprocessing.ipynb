{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJOhAF-qnZJ8",
        "colab_type": "code",
        "outputId": "eb9fbd04-1a9e-4c8f-df40-97163f0e6d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install Unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p67DKhxYnbuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import minmax_scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIFfkjzYnbpv",
        "colab_type": "code",
        "outputId": "020e762e-60cf-477f-99d6-c1d65b1bef64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5M8lubNTPO-",
        "colab_type": "code",
        "outputId": "4ec13a49-5b1f-4823-cf50-3822c771359d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_-pMytunblo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dataset from metadata file\n",
        "file=open(\"/content/drive/My Drive/ML project'20/YelpZip/metadata\")\n",
        "data=file.readlines()\n",
        "file.close()\n",
        "for ind in range(len(data)):\n",
        "    data[ind]=data[ind].split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zuFmI5QnbjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load reviews from reviewcontent file\n",
        "file2=open(\"/content/drive/My Drive/ML project'20/YelpZip/reviewContent\")\n",
        "text=file2.readlines()\n",
        "file2.close()\n",
        "for i in range(len(text)):\n",
        "    text[i]=text[i].split(\"\\t\")\n",
        "text=[t[3] for t in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBNFN3NEnbgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dataset=pd.DataFrame(data,columns=[\"user_id\",'product_id','rating','label','date'])\n",
        "Dataset[\"review_text\"]=text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYZE0jnxnbd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOE2g5WonsNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dataset['rating']=Dataset['rating'].astype('float')\n",
        "Dataset['date']=pd.to_datetime(Dataset['date'])\n",
        "Dataset['day']=Dataset['date'].dt.day_name()\n",
        "Dataset['year']=Dataset.date.dt.year\n",
        "Dataset['label']=Dataset.label.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en0fWiT7nsLE",
        "colab_type": "code",
        "outputId": "e777b4ac-5c63-4794-fa19-1edf0b756aae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Y={}\n",
        "for y in Dataset.year:\n",
        "  if y in Y:\n",
        "    Y[y]+=1\n",
        "  else:\n",
        "    Y[y]=0\n",
        "Y"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2004: 2,\n",
              " 2005: 426,\n",
              " 2006: 2261,\n",
              " 2007: 7535,\n",
              " 2008: 16781,\n",
              " 2009: 31907,\n",
              " 2010: 54578,\n",
              " 2011: 80212,\n",
              " 2012: 97961,\n",
              " 2013: 131225,\n",
              " 2014: 180674,\n",
              " 2015: 5024}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5upbwfsYBB9i",
        "colab_type": "text"
      },
      "source": [
        "### **Split Train-Valid-Test Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpkPLPNnD2r6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=Dataset[Dataset.year== 2012]\n",
        "x_real=x[x.label==1]\n",
        "x_fake=x[x.label==-1]\n",
        "train_data=x_real.sample(n=12500)\n",
        "train_data=pd.concat([train_data,x_fake.sample(n=12500)],ignore_index=True)\n",
        "train_data=train_data.sample(frac=1)\n",
        "\n",
        "x=Dataset[Dataset.year== 2013]\n",
        "x_real=x[x.label==1]\n",
        "x_fake=x[x.label==-1]\n",
        "val_data=x_real.sample(n=2500)\n",
        "val_data=pd.concat([val_data,x_fake.sample(n=2500)],ignore_index=True)\n",
        "val_data=val_data.sample(frac=1)\n",
        "\n",
        "x=Dataset[Dataset.year== 2014]\n",
        "test_data=x_real.sample(n=5000)\n",
        "x_real=x[x.label==1]\n",
        "x_fake=x[x.label==-1]\n",
        "test_data=pd.concat([test_data,x_fake.sample(n=5000)],ignore_index=True)\n",
        "test_data=test_data.sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn0RSVZctQgX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "994f8f1c-156b-44c3-b4b2-cd9895451cd8"
      },
      "source": [
        "train_data"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>label</th>\n",
              "      <th>date</th>\n",
              "      <th>review_text</th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17340</th>\n",
              "      <td>192117</td>\n",
              "      <td>3237</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2012-12-30</td>\n",
              "      <td>DELISH! Pizza was huge, flavorful and filling!...</td>\n",
              "      <td>Sunday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11072</th>\n",
              "      <td>83124</td>\n",
              "      <td>2122</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-01-21</td>\n",
              "      <td>LaVa's hummus is better than Zahav's. There. I...</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>137167</td>\n",
              "      <td>3222</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-08-10</td>\n",
              "      <td>My friend and i decided to come check this pla...</td>\n",
              "      <td>Friday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6617</th>\n",
              "      <td>51095</td>\n",
              "      <td>523</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-04-23</td>\n",
              "      <td>Oy Vey. I really really wanted to write a good...</td>\n",
              "      <td>Monday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23287</th>\n",
              "      <td>194284</td>\n",
              "      <td>2605</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2012-03-22</td>\n",
              "      <td>I heard about the wait and it was worth it. Th...</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19819</th>\n",
              "      <td>80863</td>\n",
              "      <td>3745</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2012-04-28</td>\n",
              "      <td>Delicious noodle!! Its worth the wait, if you ...</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6793</th>\n",
              "      <td>229861</td>\n",
              "      <td>4083</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-06-04</td>\n",
              "      <td>First off....this place has the cleanest bathr...</td>\n",
              "      <td>Monday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23294</th>\n",
              "      <td>19550</td>\n",
              "      <td>127</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2012-07-18</td>\n",
              "      <td>Had a Google offer for a homespun dinner for t...</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11234</th>\n",
              "      <td>122431</td>\n",
              "      <td>1907</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-08-22</td>\n",
              "      <td>The pork wonton soup was bland and the wait fo...</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13796</th>\n",
              "      <td>227446</td>\n",
              "      <td>4081</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2012-09-24</td>\n",
              "      <td>Went here for diner for the first time. Dining...</td>\n",
              "      <td>Monday</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      user_id product_id  ...        day  year\n",
              "17340  192117       3237  ...     Sunday  2012\n",
              "11072   83124       2122  ...   Saturday  2012\n",
              "440    137167       3222  ...     Friday  2012\n",
              "6617    51095        523  ...     Monday  2012\n",
              "23287  194284       2605  ...   Thursday  2012\n",
              "...       ...        ...  ...        ...   ...\n",
              "19819   80863       3745  ...   Saturday  2012\n",
              "6793   229861       4083  ...     Monday  2012\n",
              "23294   19550        127  ...  Wednesday  2012\n",
              "11234  122431       1907  ...  Wednesday  2012\n",
              "13796  227446       4081  ...     Monday  2012\n",
              "\n",
              "[25000 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7ZbX7dZUE5ll"
      },
      "source": [
        "### **Hand Crafted Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bbc2ff2e-2831-4c2f-ac42-034c4a49ceb6",
        "id": "UhfVfEp6E5lo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "\n",
        "#Train Dataset\n",
        "#Review centric features\n",
        "train_data['word_count']=train_data['review_text'].apply(lambda x: len(x.split()))\n",
        "train_data['punctuation_count']=train_data['review_text'].apply(lambda x: len(''.join(c for c in x if c in string.punctuation)))\n",
        "train_data['char_count']=train_data['review_text'].apply(lambda x: len(x))\n",
        "train_data['title_count']=train_data['review_text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
        "print(\"-----------review_centric features extracted-------------/\")\n",
        "# user-centric features\n",
        "train_data['user_id_no_of_review'] = train_data.groupby('user_id')['user_id'].transform('size')\n",
        "train_data['user_id_ave_rating'] = train_data.groupby('user_id')['rating'].transform('mean')\n",
        "train_data['user_id_ave_no_words'] = train_data.groupby('user_id')['word_count'].transform('mean')\n",
        "train_data['user_id_max_review_a_day'] = train_data.groupby(['user_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------user_centric features extracted-------------/\")\n",
        "#Product centric feature\n",
        "train_data['product_id_no_of_review']=train_data.groupby('product_id')['product_id'].transform('size')\n",
        "train_data['product_id_ave_rating']=train_data.groupby('product_id')['rating'].transform('mean')\n",
        "train_data['product_id_ave_no_of_words']=train_data.groupby('product_id')['word_count'].transform('mean')\n",
        "train_data['product_id_max_review_a_day']=train_data.groupby(['product_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------product_centric features extracted-------------/\\n\\n\")\n",
        "\n",
        "#Val dataset\n",
        "#Review centric features\n",
        "val_data['word_count']=val_data['review_text'].apply(lambda x: len(x.split()))\n",
        "val_data['punctuation_count']=val_data['review_text'].apply(lambda x: len(''.join(c for c in x if c in string.punctuation)))\n",
        "val_data['char_count']=val_data['review_text'].apply(lambda x: len(x))\n",
        "val_data['title_count']=val_data['review_text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
        "print(\"-----------review_centric features extracted-------------/\")\n",
        "# user-centric features\n",
        "val_data['user_id_no_of_review'] = val_data.groupby('user_id')['user_id'].transform('size')\n",
        "val_data['user_id_ave_rating'] = val_data.groupby('user_id')['rating'].transform('mean')\n",
        "val_data['user_id_ave_no_words'] = val_data.groupby('user_id')['word_count'].transform('mean')\n",
        "val_data['user_id_max_review_a_day'] = val_data.groupby(['user_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------user_centric features extracted-------------/\")\n",
        "#Product centric feature\n",
        "val_data['product_id_no_of_review']=val_data.groupby('product_id')['product_id'].transform('size')\n",
        "val_data['product_id_ave_rating']=val_data.groupby('product_id')['rating'].transform('mean')\n",
        "val_data['product_id_ave_no_of_words']=val_data.groupby('product_id')['word_count'].transform('mean')\n",
        "val_data['product_id_max_review_a_day']=val_data.groupby(['product_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------product_centric features extracted-------------/\\n\\n\")\n",
        "\n",
        "#Test dataset\n",
        "test_data['word_count']=test_data['review_text'].apply(lambda x: len(x.split()))\n",
        "test_data['punctuation_count']=test_data['review_text'].apply(lambda x: len(''.join(c for c in x if c in string.punctuation)))\n",
        "test_data['char_count']=test_data['review_text'].apply(lambda x: len(x))\n",
        "test_data['title_count']=test_data['review_text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
        "print(\"-----------review_centric features extracted-------------/\")\n",
        "# user-centric features\n",
        "test_data['user_id_no_of_review'] = test_data.groupby('user_id')['user_id'].transform('size')\n",
        "test_data['user_id_ave_rating'] = test_data.groupby('user_id')['rating'].transform('mean')\n",
        "test_data['user_id_ave_no_words'] = test_data.groupby('user_id')['word_count'].transform('mean')\n",
        "test_data['user_id_max_review_a_day'] = test_data.groupby(['user_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------user_centric features extracted-------------/\")\n",
        "#Product centric feature\n",
        "test_data['product_id_no_of_review']=test_data.groupby('product_id')['product_id'].transform('size')\n",
        "test_data['product_id_ave_rating']=test_data.groupby('product_id')['rating'].transform('mean')\n",
        "test_data['product_id_ave_no_of_words']=test_data.groupby('product_id')['word_count'].transform('mean')\n",
        "test_data['product_id_max_review_a_day']=test_data.groupby(['product_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------product_centric features extracted-------------/\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------review_centric features extracted-------------/\n",
            "-----------user_centric features extracted-------------/\n",
            "-----------product_centric features extracted-------------/\n",
            "\n",
            "\n",
            "-----------review_centric features extracted-------------/\n",
            "-----------user_centric features extracted-------------/\n",
            "-----------product_centric features extracted-------------/\n",
            "\n",
            "\n",
            "-----------review_centric features extracted-------------/\n",
            "-----------user_centric features extracted-------------/\n",
            "-----------product_centric features extracted-------------/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD63OU55rud9",
        "colab_type": "text"
      },
      "source": [
        "### **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D18TFeIUAH9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for expanding all the contractions in the paragraph\n",
        "with open(\"/content/drive/My Drive/ML project'20/Contractions.json\",'r') as file:\n",
        "   Contractions=json.load(file)\n",
        "c_re = re.compile('(%s)' % '|'.join(Contractions.keys()))\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return Contractions[match.group(0)]\n",
        "    return c_re.sub(replace, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ-BdKFTFPb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lowercase,remove digits,punctuations,expand contractions,stopwords and lemmatization\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train_data['review_text']=train_data['review_text'].apply(lambda x:unidecode.unidecode(x))  #Remove accented characters\n",
        "train_data['review_text']=train_data['review_text'].apply(lambda x: expandContractions(x))  # expand contractions(eg. I've to I have) \n",
        "train_data['review_text']=train_data['review_text'].apply(lambda x:x.lower())   #Convert into lowercase\n",
        "\n",
        "#Val dataset\n",
        "val_data['review_text']=val_data['review_text'].apply(lambda x:unidecode.unidecode(x))  #Remove accented characters\n",
        "val_data['review_text']=val_data['review_text'].apply(lambda x: expandContractions(x))  # expand contractions(eg. I've to I have) \n",
        "val_data['review_text']=val_data['review_text'].apply(lambda x:x.lower())   #Convert into lowercase\n",
        "\n",
        "\n",
        "# Test Dataset\n",
        "test_data['review_text']=test_data['review_text'].apply(lambda x: unidecode.unidecode(x))\n",
        "test_data['review_text']=test_data['review_text'].apply(lambda x: expandContractions(x))\n",
        "test_data['review_text']=test_data['review_text'].apply(lambda x:x.lower())   #Convert into lowercase\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfSZJUZjsWRL",
        "colab_type": "text"
      },
      "source": [
        "#### **Save Train, Valid and test csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av9Sm4nhFnb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.to_csv(\"/content/drive/My Drive/ML project'20/YelpZip/train_newb.csv\",index=False)\n",
        "val_data.to_csv(\"/content/drive/My Drive/ML project'20/YelpZip/val_newb.csv\",index=False)\n",
        "test_data.to_csv(\"/content/drive/My Drive/ML project'20/YelpZip/test_newb.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
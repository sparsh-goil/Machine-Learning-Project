{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJOhAF-qnZJ8",
        "colab_type": "code",
        "outputId": "f7271bc0-2236-4d41-93ec-f7cfe3d39f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "!pip install Unidecode"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.6MB/s \n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p67DKhxYnbuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import minmax_scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIFfkjzYnbpv",
        "colab_type": "code",
        "outputId": "42dac7df-6726-4126-ab31-4c6014ca7a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5M8lubNTPO-",
        "colab_type": "code",
        "outputId": "53ad2522-e5a0-42a4-b577-13ff69ceb9ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_-pMytunblo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load dataset from metadata file\n",
        "file=open(\"/content/drive/My Drive/ML project'20/YelpZip/metadata\")\n",
        "data=file.readlines()\n",
        "file.close()\n",
        "for ind in range(len(data)):\n",
        "    data[ind]=data[ind].split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zuFmI5QnbjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load reviews from reviewcontent file\n",
        "file2=open(\"/content/drive/My Drive/ML project'20/YelpZip/reviewContent\")\n",
        "text=file2.readlines()\n",
        "file2.close()\n",
        "for i in range(len(text)):\n",
        "    text[i]=text[i].split(\"\\t\")\n",
        "text=[t[3] for t in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBNFN3NEnbgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dataset=pd.DataFrame(data,columns=[\"user_id\",'product_id','rating','label','date'])\n",
        "Dataset[\"review_text\"]=text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYZE0jnxnbd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOE2g5WonsNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dataset['rating']=Dataset['rating'].astype('float')\n",
        "Dataset['date']=pd.to_datetime(Dataset['date'])\n",
        "Dataset['day']=Dataset['date'].dt.day_name()\n",
        "Dataset['year']=Dataset.date.dt.year\n",
        "Dataset['label']=Dataset.label.astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en0fWiT7nsLE",
        "colab_type": "code",
        "outputId": "ad9d93d2-8628-4ff6-e09c-2779d4f2174a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "Y={}\n",
        "for y in Dataset.year:\n",
        "  if y in Y:\n",
        "    Y[y]+=1\n",
        "  else:\n",
        "    Y[y]=0\n",
        "Y"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2004: 2,\n",
              " 2005: 426,\n",
              " 2006: 2261,\n",
              " 2007: 7535,\n",
              " 2008: 16781,\n",
              " 2009: 31907,\n",
              " 2010: 54578,\n",
              " 2011: 80212,\n",
              " 2012: 97961,\n",
              " 2013: 131225,\n",
              " 2014: 180674,\n",
              " 2015: 5024}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PmHakPtnsIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=Dataset[Dataset.year== 2012]\n",
        "x=x.append(Dataset[Dataset.year==2013])\n",
        "x=x.append(Dataset[Dataset.year==2014])\n",
        "train_data=x.sample(n=25000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xkzdrA3qSBv",
        "colab_type": "code",
        "outputId": "1dbac96d-f26a-43a7-d6a6-787b7d7a2a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_data=Dataset[Dataset.year==2015]\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 8)\n",
            "(5025, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mt3PX-3rjST",
        "colab_type": "text"
      },
      "source": [
        "### **Hand Crafted Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGszfoioonYu",
        "colab_type": "code",
        "outputId": "6e5ba504-935b-495f-aa00-fb7beaa81557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# X_train['upper_case_word_count']=X_train['review_text'].apply(lambda x:len([y for y in x.split() if y) )\n",
        "#Train Dataset\n",
        "#Review centric features\n",
        "train_data['word_count']=train_data['review_text'].apply(lambda x: len(x.split()))\n",
        "train_data['punctuation_count']=train_data['review_text'].apply(lambda x: len(''.join(c for c in x if c in string.punctuation)))\n",
        "train_data['char_count']=train_data['review_text'].apply(lambda x: len(x))\n",
        "train_data['title_count']=train_data['review_text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
        "print(\"-----------review_centric features extracted-------------/\")\n",
        "# user-centric features\n",
        "train_data['user_id_no_of_review'] = train_data.groupby('user_id')['user_id'].transform('size')\n",
        "train_data['user_id_ave_rating'] = train_data.groupby('user_id')['rating'].transform('mean')\n",
        "train_data['user_id_ave_no_words'] = train_data.groupby('user_id')['word_count'].transform('mean')\n",
        "train_data['user_id_max_review_a_day'] = train_data.groupby(['user_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------user_centric features extracted-------------/\")\n",
        "#Product centric feature\n",
        "train_data['product_id_no_of_review']=train_data.groupby('product_id')['product_id'].transform('size')\n",
        "train_data['product_id_ave_rating']=train_data.groupby('product_id')['rating'].transform('mean')\n",
        "train_data['product_id_ave_no_of_words']=train_data.groupby('product_id')['word_count'].transform('mean')\n",
        "train_data['product_id_max_review_a_day']=train_data.groupby(['product_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------product_centric features extracted-------------/\\n\\n\")\n",
        "\n",
        "#Test dataset\n",
        "test_data['word_count']=test_data['review_text'].apply(lambda x: len(x.split()))\n",
        "test_data['punctuation_count']=test_data['review_text'].apply(lambda x: len(''.join(c for c in x if c in string.punctuation)))\n",
        "test_data['char_count']=test_data['review_text'].apply(lambda x: len(x))\n",
        "test_data['title_count']=test_data['review_text'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n",
        "print(\"-----------review_centric features extracted-------------/\")\n",
        "# user-centric features\n",
        "test_data['user_id_no_of_review'] = test_data.groupby('user_id')['user_id'].transform('size')\n",
        "test_data['user_id_ave_rating'] = test_data.groupby('user_id')['rating'].transform('mean')\n",
        "test_data['user_id_ave_no_words'] = test_data.groupby('user_id')['word_count'].transform('mean')\n",
        "test_data['user_id_max_review_a_day'] = test_data.groupby(['user_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------user_centric features extracted-------------/\")\n",
        "#Product centric feature\n",
        "test_data['product_id_no_of_review']=test_data.groupby('product_id')['product_id'].transform('size')\n",
        "test_data['product_id_ave_rating']=test_data.groupby('product_id')['rating'].transform('mean')\n",
        "test_data['product_id_ave_no_of_words']=test_data.groupby('product_id')['word_count'].transform('mean')\n",
        "test_data['product_id_max_review_a_day']=test_data.groupby(['product_id','day'])['user_id'].transform('size')\n",
        "print(\"-----------product_centric features extracted-------------/\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------review_centric features extracted-------------/\n",
            "-----------user_centric features extracted-------------/\n",
            "-----------product_centric features extracted-------------/\n",
            "\n",
            "\n",
            "-----------review_centric features extracted-------------/\n",
            "-----------user_centric features extracted-------------/\n",
            "-----------product_centric features extracted-------------/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeKYiYwLpL_h",
        "colab_type": "code",
        "outputId": "3e1bb276-078e-4e7a-97a8-02c3d614368d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>label</th>\n",
              "      <th>date</th>\n",
              "      <th>review_text</th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>word_count</th>\n",
              "      <th>punctuation_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>title_count</th>\n",
              "      <th>user_id_no_of_review</th>\n",
              "      <th>user_id_ave_rating</th>\n",
              "      <th>user_id_ave_no_words</th>\n",
              "      <th>user_id_max_review_a_day</th>\n",
              "      <th>product_id_no_of_review</th>\n",
              "      <th>product_id_ave_rating</th>\n",
              "      <th>product_id_ave_no_of_words</th>\n",
              "      <th>product_id_max_review_a_day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>497013</th>\n",
              "      <td>29226</td>\n",
              "      <td>3183</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-13</td>\n",
              "      <td>Since my last review, I have been to Upstairs ...</td>\n",
              "      <td>Tuesday</td>\n",
              "      <td>2014</td>\n",
              "      <td>72</td>\n",
              "      <td>17</td>\n",
              "      <td>430</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>131.636364</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408918</th>\n",
              "      <td>31059</td>\n",
              "      <td>874</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2013-06-16</td>\n",
              "      <td>Just want to update that they now take reserva...</td>\n",
              "      <td>Sunday</td>\n",
              "      <td>2013</td>\n",
              "      <td>20</td>\n",
              "      <td>6</td>\n",
              "      <td>111</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>78.4</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>93.400000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120624</th>\n",
              "      <td>85382</td>\n",
              "      <td>1091</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2012-07-11</td>\n",
              "      <td>This place is over-rated in my opinion, the bu...</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>2012</td>\n",
              "      <td>52</td>\n",
              "      <td>11</td>\n",
              "      <td>283</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>121.727273</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525270</th>\n",
              "      <td>13879</td>\n",
              "      <td>4397</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2013-06-18</td>\n",
              "      <td>Ethiopian food is something that I knew nothin...</td>\n",
              "      <td>Tuesday</td>\n",
              "      <td>2013</td>\n",
              "      <td>308</td>\n",
              "      <td>65</td>\n",
              "      <td>1774</td>\n",
              "      <td>69</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>339.0</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>4.666667</td>\n",
              "      <td>133.888889</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159006</th>\n",
              "      <td>105102</td>\n",
              "      <td>4223</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-04-19</td>\n",
              "      <td>ABSOLUTELY the most terrible customer service ...</td>\n",
              "      <td>Saturday</td>\n",
              "      <td>2014</td>\n",
              "      <td>61</td>\n",
              "      <td>8</td>\n",
              "      <td>342</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>1</td>\n",
              "      <td>67</td>\n",
              "      <td>3.880597</td>\n",
              "      <td>109.164179</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       user_id  ... product_id_max_review_a_day\n",
              "497013   29226  ...                           1\n",
              "408918   31059  ...                           1\n",
              "120624   85382  ...                           3\n",
              "525270   13879  ...                           1\n",
              "159006  105102  ...                           7\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD63OU55rud9",
        "colab_type": "text"
      },
      "source": [
        "### **Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU0Y9469rt0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for expanding all the contractions in the paragraph\n",
        "with open(\"/content/drive/My Drive/ML project'20/Contractions.json\",'r') as file:\n",
        "   Contractions=json.load(file)\n",
        "c_re = re.compile('(%s)' % '|'.join(Contractions.keys()))\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return Contractions[match.group(0)]\n",
        "    return c_re.sub(replace, text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TANL5zfr18r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lowercase,remove digits,punctuations,expand contractions,stopwords and lemmatization\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train_data['review_text']=train_data['review_text'].apply(lambda x:unidecode.unidecode(x))  #Remove accented characters\n",
        "train_data['review_text']=train_data['review_text'].apply(lambda x: expandContractions(x))  # expand contractions(eg. I've to I have) \n",
        "train_data['review_text']=train_data['review_text'].apply(lambda x:x.lower())   #Convert into lowercase\n",
        "train_data['review_text']=train_data['review_text'].apply(lambda x : ''.join([c for c in x if not c.isdigit()])) #Remove numeric digits\n",
        "train_data['review_text']=train_data['review_text'].apply(lambda x:  ''.join([c for c in x if c not in string.punctuation]))  # Remove punctuations\n",
        "# train_data['review_text']=train_data['review_text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
        "\n",
        "# Test Dataset\n",
        "test_data['review_text']=test_data['review_text'].apply(lambda x: unidecode.unidecode(x))\n",
        "test_data['review_text']=test_data['review_text'].apply(lambda x: expandContractions(x))\n",
        "test_data['review_text']=test_data['review_text'].apply(lambda x:x.lower())   #Convert into lowercase\n",
        "test_data['review_text']=test_data['review_text'].apply(lambda x : ''.join([c for c in x if not c.isdigit()])) #Remove numeric digits\n",
        "test_data['review_text']=test_data['review_text'].apply(lambda x:  ''.join([c for c in x if c not in string.punctuation]))  # Remove punctuations\n",
        "# X_test['review_text']=X_test['review_text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfSZJUZjsWRL",
        "colab_type": "text"
      },
      "source": [
        "#### **Save Train and test csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBmj2YwqpO87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.to_csv(\"/content/drive/My Drive/ML project'20/YelpZip/train.csv\",index=False)\n",
        "test_data.to_csv(\"/content/drive/My Drive/ML project'20/YelpZip/test.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_ML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0V-5xDw0Hpg",
        "colab_type": "code",
        "outputId": "b2dc53cb-178f-445f-e0f6-b29b8ae8ced6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import re\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from scipy import sparse\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics import classification_report,accuracy_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00iLhqImVINw",
        "colab_type": "code",
        "outputId": "4e2adc45-b42a-4f2f-babd-557c4766d432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5M8lubNTPO-",
        "colab_type": "code",
        "outputId": "4d81d553-f5d0-4258-ddcf-6c125cb4ac89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWlPazVNA2QX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data=pd.read_csv(\"/content/drive/My Drive/ML project'20/YelpZip/train_newb.csv\")\n",
        "valid_data=pd.read_csv(\"/content/drive/My Drive/ML project'20/YelpZip/val_newb.csv\")\n",
        "test_data=pd.read_csv(\"/content/drive/My Drive/ML project'20/YelpZip/test_newb.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXUwKSkeFa47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Days={}\n",
        "i=1\n",
        "for val in train_data['day']:\n",
        "  if val not in Days:\n",
        "    Days[val]= i\n",
        "    i+=1\n",
        "train_data['day']=train_data['day'].apply(lambda x: Days[x])\n",
        "test_data['day']=test_data['day'].apply(lambda x: Days[x])\n",
        "valid_data['day']=valid_data['day'].apply(lambda x: Days[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCmlHJ1rFfS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data['label']=train_data['label'].apply(lambda x: 0 if x==-1 else 1)\n",
        "test_data['label']=test_data['label'].apply(lambda x: 0 if x==-1 else 1)\n",
        "valid_data['label']=valid_data['label'].apply(lambda x: 0 if x==-1 else 1)\n",
        "Days={}\n",
        "i=1\n",
        "for val in train_data['day']:\n",
        "  if val not in Days:\n",
        "    Days[val]= i\n",
        "    i+=1\n",
        "train_data['day']=train_data['day'].apply(lambda x: Days[x])\n",
        "test_data['day']=test_data['day'].apply(lambda x: Days[x])\n",
        "valid_data['day']=valid_data['day'].apply(lambda x: Days[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCSkvgjQA-6e",
        "colab_type": "code",
        "outputId": "19220f93-30bf-42e3-b291-a6c08c21ae04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>label</th>\n",
              "      <th>date</th>\n",
              "      <th>review_text</th>\n",
              "      <th>day</th>\n",
              "      <th>year</th>\n",
              "      <th>word_count</th>\n",
              "      <th>punctuation_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>title_count</th>\n",
              "      <th>user_id_no_of_review</th>\n",
              "      <th>user_id_ave_rating</th>\n",
              "      <th>user_id_ave_no_words</th>\n",
              "      <th>user_id_max_review_a_day</th>\n",
              "      <th>product_id_no_of_review</th>\n",
              "      <th>product_id_ave_rating</th>\n",
              "      <th>product_id_ave_no_of_words</th>\n",
              "      <th>product_id_max_review_a_day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>192117</td>\n",
              "      <td>3237</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2012-12-30</td>\n",
              "      <td>delish! pizza was huge, flavorful and filling!...</td>\n",
              "      <td>1</td>\n",
              "      <td>2012</td>\n",
              "      <td>52</td>\n",
              "      <td>8</td>\n",
              "      <td>296</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1</td>\n",
              "      <td>114</td>\n",
              "      <td>4.017544</td>\n",
              "      <td>74.684211</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>83124</td>\n",
              "      <td>2122</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-01-21</td>\n",
              "      <td>lava's hummus is better than zahav's. there. i...</td>\n",
              "      <td>2</td>\n",
              "      <td>2012</td>\n",
              "      <td>225</td>\n",
              "      <td>34</td>\n",
              "      <td>1233</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>3.666667</td>\n",
              "      <td>140.266667</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>137167</td>\n",
              "      <td>3222</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-08-10</td>\n",
              "      <td>my friend and i decided to come check this pla...</td>\n",
              "      <td>3</td>\n",
              "      <td>2012</td>\n",
              "      <td>247</td>\n",
              "      <td>21</td>\n",
              "      <td>1315</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>82.656250</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>51095</td>\n",
              "      <td>523</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2012-04-23</td>\n",
              "      <td>oy vey. i really really wanted to write a good...</td>\n",
              "      <td>4</td>\n",
              "      <td>2012</td>\n",
              "      <td>210</td>\n",
              "      <td>52</td>\n",
              "      <td>1184</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>210.0</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>3.111111</td>\n",
              "      <td>107.611111</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>194284</td>\n",
              "      <td>2605</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2012-03-22</td>\n",
              "      <td>i heard about the wait and it was worth it. th...</td>\n",
              "      <td>5</td>\n",
              "      <td>2012</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>95</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>25.5</td>\n",
              "      <td>2</td>\n",
              "      <td>117</td>\n",
              "      <td>3.837607</td>\n",
              "      <td>124.427350</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  product_id  ...  product_id_ave_no_of_words  product_id_max_review_a_day\n",
              "0   192117        3237  ...                   74.684211                           14\n",
              "1    83124        2122  ...                  140.266667                            3\n",
              "2   137167        3222  ...                   82.656250                            4\n",
              "3    51095         523  ...                  107.611111                            6\n",
              "4   194284        2605  ...                  124.427350                           15\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWzqbVZfDPNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=train_data.drop(['user_id','product_id','year','date','label','rating','review_text'],axis=1)\n",
        "y_train=train_data.label.values\n",
        "X_test=test_data.drop(['user_id','product_id','year','date','label','rating','review_text'],axis=1)\n",
        "y_test=test_data.label.values\n",
        "X_valid=valid_data.drop(['user_id','product_id','year','date','label','rating','review_text'],axis=1)\n",
        "y_valid=valid_data.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQMfJBB4_S3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('mode.chained_assignment', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O1lyy3VQ9c2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat=1000\n",
        "count_vect_1 = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1))\n",
        "count_vect_1.fit(train_data.review_text.values)\n",
        "pos=count_vect_1.transform(train_data.review_text[train_data.label==1])\n",
        "neg=count_vect_1.transform(train_data.review_text[train_data.label==0])\n",
        "val=(neg.sum(axis=0)-pos.sum(axis=0))\n",
        "ind_2=np.squeeze(np.asarray(val)).argsort()[-feat:][::-1]\n",
        "va_1l=(pos.sum(axis=0)-neg.sum(axis=0))\n",
        "ind_1=np.squeeze(np.asarray(va_1l)).argsort()[-feat:][::-1]\n",
        "ind=np.concatenate((ind_1,ind_2))\n",
        "voc=np.array(count_vect_1.get_feature_names())\n",
        "vocab=dict(np.ndenumerate(voc[ind]))\n",
        "xtrain_count_1=count_vect_1.transform(train_data.review_text)\n",
        "xtrain_count_1=xtrain_count_1[:,ind]\n",
        "xtest_count_1=count_vect_1.transform(test_data.review_text)\n",
        "xtest_count_1=xtest_count_1[:,ind]\n",
        "xvalid_count_1 =count_vect_1.transform(valid_data.review_text)\n",
        "xvalid_count_1=xvalid_count_1[:,ind]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAXkLAdMmNv1",
        "colab_type": "code",
        "outputId": "fe4885aa-2d05-41f4-d1b4-4be389c12526",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "xtrain_count_1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<25000x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 1362751 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x47lLHyZmIVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert text features into \n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "# Convert into Tensor sparse matrix\n",
        "values = xtrain_count_1.data\n",
        "indices = np.vstack((xtrain_count_1.tocoo().row, xtrain_count_1.tocoo().col))\n",
        "\n",
        "i = torch.LongTensor(indices)\n",
        "v = torch.FloatTensor(values)\n",
        "shape = xtrain_count_1.shape\n",
        "\n",
        "train_features_tensor=torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
        "\n",
        "#Valid\n",
        "values = xvalid_count_1.data\n",
        "indices = np.vstack((xvalid_count_1.tocoo().row, xvalid_count_1.tocoo().col))\n",
        "\n",
        "i = torch.LongTensor(indices)\n",
        "v = torch.FloatTensor(values)\n",
        "shape = xtrain_count_1.shape\n",
        "\n",
        "valid_features_tensor=torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
        "\n",
        "\n",
        "#test\n",
        "values = xtest_count_1.data\n",
        "indices = np.vstack((xtest_count_1.tocoo().row, xtest_count_1.tocoo().col))\n",
        "\n",
        "i = torch.LongTensor(indices)\n",
        "v = torch.FloatTensor(values)\n",
        "shape = xtrain_count_1.shape\n",
        "\n",
        "test_features_tensor=torch.sparse.FloatTensor(i, v, torch.Size(shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9WpCengjTmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "class MycustomDataset(Dataset):\n",
        "  def __init__(self,text_features,meta_features,y):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.metafeatures=meta_features\n",
        "    self.textfeatures=text_features\n",
        "    self.y=y\n",
        "  def __len__(self):\n",
        "    return len(self.y)\n",
        "  def __getitem__(self,idx):\n",
        "    return self.textfeatures[idx],self.metafeatures[idx],self.y[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyUqUYvgU3_u",
        "colab_type": "code",
        "outputId": "46b08386-4794-4491-d6bb-b8e4a6067b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "train_features_tensor[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(indices=tensor([[ 401,  298, 1057,    2,   42,    0,   82, 1003, 1002,\n",
              "                         190, 1007, 1006,   95,  787,    4, 1184,  254,   54,\n",
              "                           5,    3,  201, 1001, 1133,  142,  185, 1009,  488,\n",
              "                          36,    6, 1581,   46,    8, 1040,    1]]),\n",
              "       values=tensor([0.1710, 0.1401, 0.0889, 0.1770, 0.0510, 0.0839, 0.1306,\n",
              "                      0.1172, 0.0846, 0.1533, 0.0700, 0.1322, 0.0894, 0.1747,\n",
              "                      0.1121, 0.2214, 0.1210, 0.0876, 0.0557, 0.0979, 0.1525,\n",
              "                      0.2298, 0.1334, 0.1700, 0.1917, 0.1556, 0.2123, 0.1136,\n",
              "                      0.0626, 0.1667, 0.0863, 0.1739, 0.1208, 0.0477]),\n",
              "       size=(2000,), nnz=34, layout=torch.sparse_coo)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mkNcPwYHHgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset=MycustomDataset(train_features_tensor,torch.tensor(X_train.values),torch.tensor(y_train))\n",
        "train_iterator=DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
        "valid_dataset=MycustomDataset(valid_features_tensor,torch.tensor(X_valid.values),torch.tensor(y_valid))\n",
        "valid_iterator=DataLoader(valid_dataset,batch_size=256,shuffle=True)\n",
        "test_dataset=MycustomDataset(test_features_tensor,torch.tensor(X_test.values),torch.tensor(y_test))\n",
        "test_iterator=DataLoader(test_dataset,batch_size=256,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2pKZ4NdVNJx",
        "colab_type": "code",
        "outputId": "704f7aec-5c0c-4838-9932-21d339c9bccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_iterator)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpTSUE9mJTk2",
        "colab_type": "code",
        "outputId": "4411e571-5a78-4aef-cff1-333a8743cd1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "for i in valid_iterator:\n",
        "  print(i[0])\n",
        "  print(i[0].shape)\n",
        "  print(i[1].shape)\n",
        "  print(i[2].shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(indices=tensor([[  0,   0,   0,  ..., 255, 255, 255],\n",
            "                       [ 92,  44,   7,  ..., 172,   1,  86]]),\n",
            "       values=tensor([0.1067, 0.1741, 0.0624,  ..., 0.0299, 0.0584, 0.0407]),\n",
            "       size=(256, 2000), nnz=13345, layout=torch.sparse_coo)\n",
            "torch.Size([256, 2000])\n",
            "torch.Size([256, 13])\n",
            "torch.Size([256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N9X9C0w0tHC",
        "colab_type": "text"
      },
      "source": [
        "### **Neural Network Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1eEsPIZ0UI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "from torchtext import data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRz7Chnm1XaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self,batch_size,hidden_size1,hidden_size2,hidden_size3,hidden_size4,outputsize,metadata_length,text_length,keep_probab):\n",
        "    super().__init__()\n",
        "    self.input_size=batch_size\n",
        "    self.feature_size_text= text_length #1000\n",
        "    self.feature_size_metadata=metadata_length\n",
        "    self.hidden_size1=hidden_size1\n",
        "    self.hidden_size2=hidden_size2\n",
        "    self.hidden_size3=hidden_size3\n",
        "    self.hidden_size4=hidden_size4\n",
        "    self.output_size=output_size\n",
        "    self.layer1=nn.Linear(text_length,self.hidden_size1)\n",
        "    self.layer2=nn.Linear(self.hidden_size1,self.hidden_size1)\n",
        "    self.layer3=nn.Linear(self.hidden_size1,self.hidden_size2)\n",
        "    self.layer4=nn.Linear(metadata_length,self.hidden_size3)\n",
        "    self.layer5=nn.Linear(self.hidden_size3,self.hidden_size3)\n",
        "    self.layer6=nn.Linear(self.hidden_size3,self.hidden_size4)\n",
        "    self.out_layer=nn.Linear(self.hidden_size4+self.hidden_size2,self.output_size)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.dropout=nn.Dropout(keep_probab)\n",
        "    self.softmax=nn.Softmax()\n",
        "\n",
        "  def forward(self,input_text,input_features):\n",
        "    # 3 hidden layer neural network for review text.\n",
        "    z=self.layer1(input_text)\n",
        "    z=self.dropout(z)\n",
        "    z=self.relu(z)\n",
        "    z1=self.layer2(z)\n",
        "    z1=self.dropout(z1)\n",
        "    z1=self.relu(z1)\n",
        "    z2=self.layer3(z1)\n",
        "    z2=self.relu(z2)\n",
        "    out1=self.dropout(z2)\n",
        "\n",
        "    #3 hidden layer neural network for metadata features\n",
        "    z3=self.layer4(input_features)\n",
        "    z3=self.dropout(z3)\n",
        "    z3=self.relu(z3)\n",
        "    z4=self.layer5(z3)\n",
        "    z4=self.dropout(z3)\n",
        "    z4=self.relu(z4)\n",
        "    z5=self.layer6(z4)\n",
        "    z5=self.relu(z5)\n",
        "    out2=self.dropout(z5)\n",
        "    \n",
        "    #Output Layer\n",
        "    self.out=torch.cat((out1,out2),dim=1)\n",
        "    # self.out=self.dropout(self.out)\n",
        "    self.out=self.out_layer(self.out)\n",
        "    output=self.softmax(self.out)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFwhaLmqhQqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=256\n",
        "hidden_size1=1000\n",
        "hidden_size2=400\n",
        "hidden_size3=10\n",
        "hidden_size4=5\n",
        "text_length=2000\n",
        "metadata_length=13\n",
        "output_size=2\n",
        "keep_probab=0.2\n",
        "NN=NeuralNetwork(batch_size,hidden_size1,hidden_size2,hidden_size3,hidden_size4,output_size,metadata_length,text_length,keep_probab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "807mEF9Zaklm",
        "colab_type": "code",
        "outputId": "38a36d79-dc1d-4dde-f759-688fd5797ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#architecture\n",
        "print(NN)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(NN):\n",
        "    return sum(p.numel() for p in NN.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(NN):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (layer1): Linear(in_features=2000, out_features=1000, bias=True)\n",
            "  (layer2): Linear(in_features=1000, out_features=1000, bias=True)\n",
            "  (layer3): Linear(in_features=1000, out_features=400, bias=True)\n",
            "  (layer4): Linear(in_features=13, out_features=10, bias=True)\n",
            "  (layer5): Linear(in_features=10, out_features=10, bias=True)\n",
            "  (layer6): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (out_layer): Linear(in_features=405, out_features=2, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n",
            "The model has 3,403,517 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23pEbHciLEA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check whether cuda is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#define optimizer and loss\n",
        "optimizer = optim.Adam(NN.parameters())\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "#define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds)\n",
        "    \n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "#push to cuda if available\n",
        "# model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daM8HlsuLD9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train function for Bi-LSTM\n",
        "\n",
        "def train(NN, iterator, optimizer, criterion):\n",
        "    \n",
        "    #initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    predicted=[]\n",
        "    truth=[]\n",
        "    #set the model in training phase\n",
        "    NN.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        #resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        #retrieve text and no. of words\n",
        "        # input_text = batch[-14:]\n",
        "        # metadata=batch[0:10000]\n",
        "        #convert to 1D tensor\n",
        "        predictions = NN(batch[0], batch[1].float())  \n",
        "        \n",
        "        #compute the loss\n",
        "        loss = criterion(predictions[:,1], batch[2].float())        \n",
        "        \n",
        "        #compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions[:,1], batch[2])   \n",
        "        \n",
        "        #backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        #update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        #loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        predicted += list(predictions[:,1].detach().cpu().numpy().round())\n",
        "        truth += list(batch[2].detach().cpu().numpy())\n",
        "\n",
        "    f1 = f1_score(truth, predicted)\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator),f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcEcWvUop-cw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    #initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    predicted=[]\n",
        "    truth=[]\n",
        "    model.eval()\n",
        "    \n",
        "    #deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            #retrieve text and no. of words\n",
        "            # text, text_lengths = batch.headline\n",
        "            \n",
        "            #convert to 1d tensor\n",
        "            predictions = model(batch[0],batch[1].float())\n",
        "            #compute loss and accuracy\n",
        "            loss = criterion(predictions[:,1], batch[2].float())\n",
        "            acc = binary_accuracy(predictions[:,1], batch[2])\n",
        "            #keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "            predicted += list(predictions[:,1].detach().cpu().numpy().round())\n",
        "            truth += list(batch[2].detach().cpu().numpy())\n",
        "            \n",
        "    f1 = f1_score(truth, predicted)\n",
        "    report=classification_report(truth,predicted)\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1,report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O0Q0PW4MsXf",
        "colab_type": "code",
        "outputId": "66183f7a-9fbe-45cc-a122-ecd250c6747b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "source": [
        "#p=0.2\n",
        "N_EPOCHS = 3\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    #train the model\n",
        "    train_loss, train_acc,f1_train = train(NN, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    #evaluate the model\n",
        "    valid_loss,valid_acc,f1_valid,classification__report=evaluate(NN,valid_iterator,criterion)\n",
        "    test_loss, test_acc,f1_test,classification__report = evaluate(NN, test_iterator, criterion)\n",
        "\n",
        "    #save the best model\n",
        "    if test_loss < best_valid_loss:\n",
        "        best_valid_loss = test_loss\n",
        "        torch.save(NN.state_dict(), 'saved_weights_cnn.pt')\n",
        "    print(\"For Epoch {}\".format(epoch+1))\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}   | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f}   | Valid Acc: {valid_acc*100:.2f}%')\n",
        "    print(f'\\tTest  Loss: {test_loss:.3f}   |  Test  Acc: {test_acc*100:.2f}%')\n",
        "    print(f'\\tF1 train  : {f1_train:.4f}    | F1 test  : {f1_test:.4f} | F1 Valid  : {f1_valid:.4f}')\n",
        "    print('\\033[1m'+\"Classification Report\"+'\\033[0m')\n",
        "    print(classification__report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "For Epoch 1\n",
            "\tTrain Loss: 0.647   | Train Acc: 62.52%\n",
            "\tValid Loss: 0.626   | Valid Acc: 64.63%\n",
            "\tTest  Loss: 0.612   |  Test  Acc: 65.49%\n",
            "\tF1 train  : 0.6027    | F1 test  : 0.6300 | F1 Valid  : 0.6120\n",
            "\u001b[1mClassification Report\u001b[0m\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.74      0.68      5000\n",
            "           1       0.69      0.58      0.63      5000\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.66      0.66      0.66     10000\n",
            "weighted avg       0.66      0.66      0.66     10000\n",
            "\n",
            "For Epoch 2\n",
            "\tTrain Loss: 0.553   | Train Acc: 72.18%\n",
            "\tValid Loss: 0.632   | Valid Acc: 65.21%\n",
            "\tTest  Loss: 0.616   |  Test  Acc: 66.23%\n",
            "\tF1 train  : 0.7163    | F1 test  : 0.6588 | F1 Valid  : 0.6456\n",
            "\u001b[1mClassification Report\u001b[0m\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.68      0.67      5000\n",
            "           1       0.67      0.65      0.66      5000\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.66      0.66      0.66     10000\n",
            "weighted avg       0.66      0.66      0.66     10000\n",
            "\n",
            "For Epoch 3\n",
            "\tTrain Loss: 0.432   | Train Acc: 80.41%\n",
            "\tValid Loss: 0.737   | Valid Acc: 64.03%\n",
            "\tTest  Loss: 0.713   |  Test  Acc: 64.56%\n",
            "\tF1 train  : 0.8009    | F1 test  : 0.6546 | F1 Valid  : 0.6456\n",
            "\u001b[1mClassification Report\u001b[0m\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.63      0.64      5000\n",
            "           1       0.64      0.67      0.65      5000\n",
            "\n",
            "    accuracy                           0.65     10000\n",
            "   macro avg       0.65      0.65      0.65     10000\n",
            "weighted avg       0.65      0.65      0.65     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}